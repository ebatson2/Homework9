[
  {
    "objectID": "Homework9.html",
    "href": "Homework9.html",
    "title": "Homework9",
    "section": "",
    "text": "Reading Data\n\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(corrr)\nlibrary(tidymodels)\nlibrary(rpart)\nlibrary(baguette)\n\ndf &lt;- readr::read_csv(\"SeoulBikeData.csv\", locale=locale(encoding=\"latin1\"))\nhead(df)\n\n\n\nEDA\nFirst, let’s rename the columns to make them easier to work with:\n\ncolnames(df) &lt;- gsub(\" \", \"_\", colnames(df))\n\ndf &lt;- df |&gt;\n  rename(\"Temperature_C\"=\"Temperature(°C)\") |&gt;\n  rename(\"Humidity_Percent\"=\"Humidity(%)\") |&gt;\n  rename(\"Wind_speed_m_per_s\"=\"Wind_speed_(m/s)\") |&gt;\n  rename(\"Visibility_10m\"=\"Visibility_(10m)\") |&gt;\n  rename(\"Dew_point_temperature_C\"=\"Dew_point_temperature(°C)\") |&gt;\n  rename(\"Solar_Radiation_MJ_per_sq_m\"=\"Solar_Radiation_(MJ/m2)\") |&gt;\n  rename(\"Rainfall_mm\"=\"Rainfall(mm)\") |&gt;\n  rename(\"Snowfall_cm\"=\"Snowfall_(cm)\")\n\nhead(df)\n\n# A tibble: 6 × 14\n  Date       Rented_Bike_Count  Hour Temperature_C Humidity_Percent\n  &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n1 01/12/2017               254     0          -5.2               37\n2 01/12/2017               204     1          -5.5               38\n3 01/12/2017               173     2          -6                 39\n4 01/12/2017               107     3          -6.2               40\n5 01/12/2017                78     4          -6                 36\n6 01/12/2017               100     5          -6.4               37\n# ℹ 9 more variables: Wind_speed_m_per_s &lt;dbl&gt;, Visibility_10m &lt;dbl&gt;,\n#   Dew_point_temperature_C &lt;dbl&gt;, Solar_Radiation_MJ_per_sq_m &lt;dbl&gt;,\n#   Rainfall_mm &lt;dbl&gt;, Snowfall_cm &lt;dbl&gt;, Seasons &lt;chr&gt;, Holiday &lt;chr&gt;,\n#   Functioning_Day &lt;chr&gt;\n\n\nLet’s do some checking to validate the dataset before we build a model with it:\n\n# check for missing values\nsum_na &lt;- function(column){\n sum(is.na(column))\n}\n\nna_counts &lt;- df |&gt;\n summarize(across(everything(), sum_na))\n\nna_counts\n\n# A tibble: 1 × 14\n   Date Rented_Bike_Count  Hour Temperature_C Humidity_Percent\n  &lt;int&gt;             &lt;int&gt; &lt;int&gt;         &lt;int&gt;            &lt;int&gt;\n1     0                 0     0             0                0\n# ℹ 9 more variables: Wind_speed_m_per_s &lt;int&gt;, Visibility_10m &lt;int&gt;,\n#   Dew_point_temperature_C &lt;int&gt;, Solar_Radiation_MJ_per_sq_m &lt;int&gt;,\n#   Rainfall_mm &lt;int&gt;, Snowfall_cm &lt;int&gt;, Seasons &lt;int&gt;, Holiday &lt;int&gt;,\n#   Functioning_Day &lt;int&gt;\n\n\nThere are no NA values in our data. What are the values seen?\n\ncat_vars &lt;- c(\"Date\", \"Seasons\", \"Holiday\", \"Functioning_Day\")\n\nfor (var in cat_vars){\n  print(head(unique(df[var])))\n}\n\n# A tibble: 6 × 1\n  Date      \n  &lt;chr&gt;     \n1 01/12/2017\n2 02/12/2017\n3 03/12/2017\n4 04/12/2017\n5 05/12/2017\n6 06/12/2017\n# A tibble: 4 × 1\n  Seasons\n  &lt;chr&gt;  \n1 Winter \n2 Spring \n3 Summer \n4 Autumn \n# A tibble: 2 × 1\n  Holiday   \n  &lt;chr&gt;     \n1 No Holiday\n2 Holiday   \n# A tibble: 2 × 1\n  Functioning_Day\n  &lt;chr&gt;          \n1 Yes            \n2 No             \n\ntable(df$Seasons)\n\n\nAutumn Spring Summer Winter \n  2184   2208   2208   2160 \n\ntable(df$Holiday)\n\n\n   Holiday No Holiday \n       432       8328 \n\ntable(df$Functioning_Day)\n\n\n  No  Yes \n 295 8465 \n\n\nConvert the Date column to actual date type and other categorical variables into factors:\n\ndf &lt;- df |&gt;\n  mutate(Date = parse_date_time(Date, \"dmy\")) |&gt;\n  mutate(Seasons = as.factor(Seasons)) |&gt;\n  mutate(Holiday = as.factor(Holiday))|&gt;\n  mutate(Functioning_Day = as.factor(Functioning_Day))\n\ndf\n\n# A tibble: 8,760 × 14\n   Date                Rented_Bike_Count  Hour Temperature_C Humidity_Percent\n   &lt;dttm&gt;                          &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n 1 2017-12-01 00:00:00               254     0          -5.2               37\n 2 2017-12-01 00:00:00               204     1          -5.5               38\n 3 2017-12-01 00:00:00               173     2          -6                 39\n 4 2017-12-01 00:00:00               107     3          -6.2               40\n 5 2017-12-01 00:00:00                78     4          -6                 36\n 6 2017-12-01 00:00:00               100     5          -6.4               37\n 7 2017-12-01 00:00:00               181     6          -6.6               35\n 8 2017-12-01 00:00:00               460     7          -7.4               38\n 9 2017-12-01 00:00:00               930     8          -7.6               37\n10 2017-12-01 00:00:00               490     9          -6.5               27\n# ℹ 8,750 more rows\n# ℹ 9 more variables: Wind_speed_m_per_s &lt;dbl&gt;, Visibility_10m &lt;dbl&gt;,\n#   Dew_point_temperature_C &lt;dbl&gt;, Solar_Radiation_MJ_per_sq_m &lt;dbl&gt;,\n#   Rainfall_mm &lt;dbl&gt;, Snowfall_cm &lt;dbl&gt;, Seasons &lt;fct&gt;, Holiday &lt;fct&gt;,\n#   Functioning_Day &lt;fct&gt;\n\n\nCalculate summaries to check in with numeric variables:\n\n# function for finding measures of center and spread\nfind_center_and_spread &lt;- function(df) {\n  return(df|&gt;\n    summarize(across(where(is.numeric), \n               list(\"mean\" = mean, \"median\" = median, \"sd\"=sd, \"IQR\"=IQR), \n               .names = \"{.fn}_{.col}\")))\n}\n\n# find centers and spread for all numeric vars\nfind_center_and_spread(df)\n\n# A tibble: 1 × 40\n  mean_Rented_Bike_Count median_Rented_Bike_Count sd_Rented_Bike_Count\n                   &lt;dbl&gt;                    &lt;dbl&gt;                &lt;dbl&gt;\n1                   705.                     504.                 645.\n# ℹ 37 more variables: IQR_Rented_Bike_Count &lt;dbl&gt;, mean_Hour &lt;dbl&gt;,\n#   median_Hour &lt;dbl&gt;, sd_Hour &lt;dbl&gt;, IQR_Hour &lt;dbl&gt;, mean_Temperature_C &lt;dbl&gt;,\n#   median_Temperature_C &lt;dbl&gt;, sd_Temperature_C &lt;dbl&gt;,\n#   IQR_Temperature_C &lt;dbl&gt;, mean_Humidity_Percent &lt;dbl&gt;,\n#   median_Humidity_Percent &lt;dbl&gt;, sd_Humidity_Percent &lt;dbl&gt;,\n#   IQR_Humidity_Percent &lt;dbl&gt;, mean_Wind_speed_m_per_s &lt;dbl&gt;,\n#   median_Wind_speed_m_per_s &lt;dbl&gt;, sd_Wind_speed_m_per_s &lt;dbl&gt;, …\n\n\nNow that we have cleaned up our column names and datatypes, as well as validated the values in the dataset and confirmed there are no missing values, let’s look more into the relationships between our target variable (Rented_Bike_Count) and other variables.\n\n# summarize across categorical variables\n# function for finding measures of center and spread\nfind_center_and_spread_grouped &lt;- function(df, group) {\n  return(df|&gt;\n    group_by({{group}}) |&gt;\n    summarize(across(where(is.numeric), \n               list(\"mean\" = mean, \"median\" = median, \"sd\"=sd, \"IQR\"=IQR), \n               .names = \"{.fn}_{.col}\")))\n}\n\n# find centers and spread for all numeric vars, grouped by cat vars\nfind_center_and_spread_grouped(df, Seasons)\n\n# A tibble: 4 × 41\n  Seasons mean_Rented_Bike_Count median_Rented_Bike_Count sd_Rented_Bike_Count\n  &lt;fct&gt;                    &lt;dbl&gt;                    &lt;dbl&gt;                &lt;dbl&gt;\n1 Autumn                    820.                     764.                 651.\n2 Spring                    730.                     583                  622.\n3 Summer                   1034.                     906.                 690.\n4 Winter                    226.                     203                  150.\n# ℹ 37 more variables: IQR_Rented_Bike_Count &lt;dbl&gt;, mean_Hour &lt;dbl&gt;,\n#   median_Hour &lt;dbl&gt;, sd_Hour &lt;dbl&gt;, IQR_Hour &lt;dbl&gt;, mean_Temperature_C &lt;dbl&gt;,\n#   median_Temperature_C &lt;dbl&gt;, sd_Temperature_C &lt;dbl&gt;,\n#   IQR_Temperature_C &lt;dbl&gt;, mean_Humidity_Percent &lt;dbl&gt;,\n#   median_Humidity_Percent &lt;dbl&gt;, sd_Humidity_Percent &lt;dbl&gt;,\n#   IQR_Humidity_Percent &lt;dbl&gt;, mean_Wind_speed_m_per_s &lt;dbl&gt;,\n#   median_Wind_speed_m_per_s &lt;dbl&gt;, sd_Wind_speed_m_per_s &lt;dbl&gt;, …\n\nfind_center_and_spread_grouped(df, Holiday)\n\n# A tibble: 2 × 41\n  Holiday    mean_Rented_Bike_Count median_Rented_Bike_Co…¹ sd_Rented_Bike_Count\n  &lt;fct&gt;                       &lt;dbl&gt;                   &lt;dbl&gt;                &lt;dbl&gt;\n1 Holiday                      500.                    240                  571.\n2 No Holiday                   715.                    524.                 647.\n# ℹ abbreviated name: ¹​median_Rented_Bike_Count\n# ℹ 37 more variables: IQR_Rented_Bike_Count &lt;dbl&gt;, mean_Hour &lt;dbl&gt;,\n#   median_Hour &lt;dbl&gt;, sd_Hour &lt;dbl&gt;, IQR_Hour &lt;dbl&gt;, mean_Temperature_C &lt;dbl&gt;,\n#   median_Temperature_C &lt;dbl&gt;, sd_Temperature_C &lt;dbl&gt;,\n#   IQR_Temperature_C &lt;dbl&gt;, mean_Humidity_Percent &lt;dbl&gt;,\n#   median_Humidity_Percent &lt;dbl&gt;, sd_Humidity_Percent &lt;dbl&gt;,\n#   IQR_Humidity_Percent &lt;dbl&gt;, mean_Wind_speed_m_per_s &lt;dbl&gt;, …\n\nfind_center_and_spread_grouped(df, Functioning_Day)\n\n# A tibble: 2 × 41\n  Functioning_Day mean_Rented_Bike_Count median_Rented_Bike_Count\n  &lt;fct&gt;                            &lt;dbl&gt;                    &lt;dbl&gt;\n1 No                                  0                         0\n2 Yes                               729.                      542\n# ℹ 38 more variables: sd_Rented_Bike_Count &lt;dbl&gt;, IQR_Rented_Bike_Count &lt;dbl&gt;,\n#   mean_Hour &lt;dbl&gt;, median_Hour &lt;dbl&gt;, sd_Hour &lt;dbl&gt;, IQR_Hour &lt;dbl&gt;,\n#   mean_Temperature_C &lt;dbl&gt;, median_Temperature_C &lt;dbl&gt;,\n#   sd_Temperature_C &lt;dbl&gt;, IQR_Temperature_C &lt;dbl&gt;,\n#   mean_Humidity_Percent &lt;dbl&gt;, median_Humidity_Percent &lt;dbl&gt;,\n#   sd_Humidity_Percent &lt;dbl&gt;, IQR_Humidity_Percent &lt;dbl&gt;,\n#   mean_Wind_speed_m_per_s &lt;dbl&gt;, median_Wind_speed_m_per_s &lt;dbl&gt;, …\n\n\nNotes:\n\nAll Rented_Bike_Count values are 0 for Functioning_Day=No. Since the effects of other variables cannot influence the rentals on these days, we can filter them out to get a better picture of how the other variables influence the target.\nRented_Bike_Count is lower for holidays.\nRented_Bike_Count is highest in summer, followed by autumn, spring, and winter.\n\nSubset the data to filter out Functioning_Day=No:\n\ndf_filtered &lt;- df |&gt;\n  filter(Functioning_Day=='Yes')\n\ndf\n\n# A tibble: 8,760 × 14\n   Date                Rented_Bike_Count  Hour Temperature_C Humidity_Percent\n   &lt;dttm&gt;                          &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n 1 2017-12-01 00:00:00               254     0          -5.2               37\n 2 2017-12-01 00:00:00               204     1          -5.5               38\n 3 2017-12-01 00:00:00               173     2          -6                 39\n 4 2017-12-01 00:00:00               107     3          -6.2               40\n 5 2017-12-01 00:00:00                78     4          -6                 36\n 6 2017-12-01 00:00:00               100     5          -6.4               37\n 7 2017-12-01 00:00:00               181     6          -6.6               35\n 8 2017-12-01 00:00:00               460     7          -7.4               38\n 9 2017-12-01 00:00:00               930     8          -7.6               37\n10 2017-12-01 00:00:00               490     9          -6.5               27\n# ℹ 8,750 more rows\n# ℹ 9 more variables: Wind_speed_m_per_s &lt;dbl&gt;, Visibility_10m &lt;dbl&gt;,\n#   Dew_point_temperature_C &lt;dbl&gt;, Solar_Radiation_MJ_per_sq_m &lt;dbl&gt;,\n#   Rainfall_mm &lt;dbl&gt;, Snowfall_cm &lt;dbl&gt;, Seasons &lt;fct&gt;, Holiday &lt;fct&gt;,\n#   Functioning_Day &lt;fct&gt;\n\ndf_filtered\n\n# A tibble: 8,465 × 14\n   Date                Rented_Bike_Count  Hour Temperature_C Humidity_Percent\n   &lt;dttm&gt;                          &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n 1 2017-12-01 00:00:00               254     0          -5.2               37\n 2 2017-12-01 00:00:00               204     1          -5.5               38\n 3 2017-12-01 00:00:00               173     2          -6                 39\n 4 2017-12-01 00:00:00               107     3          -6.2               40\n 5 2017-12-01 00:00:00                78     4          -6                 36\n 6 2017-12-01 00:00:00               100     5          -6.4               37\n 7 2017-12-01 00:00:00               181     6          -6.6               35\n 8 2017-12-01 00:00:00               460     7          -7.4               38\n 9 2017-12-01 00:00:00               930     8          -7.6               37\n10 2017-12-01 00:00:00               490     9          -6.5               27\n# ℹ 8,455 more rows\n# ℹ 9 more variables: Wind_speed_m_per_s &lt;dbl&gt;, Visibility_10m &lt;dbl&gt;,\n#   Dew_point_temperature_C &lt;dbl&gt;, Solar_Radiation_MJ_per_sq_m &lt;dbl&gt;,\n#   Rainfall_mm &lt;dbl&gt;, Snowfall_cm &lt;dbl&gt;, Seasons &lt;fct&gt;, Holiday &lt;fct&gt;,\n#   Functioning_Day &lt;fct&gt;\n\n\nCombine the rows into 1 row for each day:\n\nday_df &lt;- df_filtered |&gt;\n  group_by(Date, Seasons, Holiday) |&gt;\n  summarise(across(c(Rented_Bike_Count, Rainfall_mm, Snowfall_cm), sum), \n            across(c(Temperature_C, Humidity_Percent, Wind_speed_m_per_s, Visibility_10m, Dew_point_temperature_C, Solar_Radiation_MJ_per_sq_m), \n                   mean),\n            .groups='drop')\n\nhead(day_df)\n\n# A tibble: 6 × 12\n  Date                Seasons Holiday  Rented_Bike_Count Rainfall_mm Snowfall_cm\n  &lt;dttm&gt;              &lt;fct&gt;   &lt;fct&gt;                &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 2017-12-01 00:00:00 Winter  No Holi…              9539         0           0  \n2 2017-12-02 00:00:00 Winter  No Holi…              8523         0           0  \n3 2017-12-03 00:00:00 Winter  No Holi…              7222         4           0  \n4 2017-12-04 00:00:00 Winter  No Holi…              8729         0.1         0  \n5 2017-12-05 00:00:00 Winter  No Holi…              8307         0           0  \n6 2017-12-06 00:00:00 Winter  No Holi…              6669         1.3         8.6\n# ℹ 6 more variables: Temperature_C &lt;dbl&gt;, Humidity_Percent &lt;dbl&gt;,\n#   Wind_speed_m_per_s &lt;dbl&gt;, Visibility_10m &lt;dbl&gt;,\n#   Dew_point_temperature_C &lt;dbl&gt;, Solar_Radiation_MJ_per_sq_m &lt;dbl&gt;\n\n\nNow that the data has been combined for each day, we’ll take another look at our summary statistics for categorical and numerical variables:\n\n# categorical vars:\ntable(day_df$Seasons)\n\n\nAutumn Spring Summer Winter \n    81     90     92     90 \n\ntable(day_df$Holiday)\n\n\n   Holiday No Holiday \n        17        336 \n\n# numerical vars:\nfind_center_and_spread(day_df)\n\n# A tibble: 1 × 36\n  mean_Rented_Bike_Count median_Rented_Bike_Count sd_Rented_Bike_Count\n                   &lt;dbl&gt;                    &lt;dbl&gt;                &lt;dbl&gt;\n1                 17485.                    18563                9937.\n# ℹ 33 more variables: IQR_Rented_Bike_Count &lt;dbl&gt;, mean_Rainfall_mm &lt;dbl&gt;,\n#   median_Rainfall_mm &lt;dbl&gt;, sd_Rainfall_mm &lt;dbl&gt;, IQR_Rainfall_mm &lt;dbl&gt;,\n#   mean_Snowfall_cm &lt;dbl&gt;, median_Snowfall_cm &lt;dbl&gt;, sd_Snowfall_cm &lt;dbl&gt;,\n#   IQR_Snowfall_cm &lt;dbl&gt;, mean_Temperature_C &lt;dbl&gt;,\n#   median_Temperature_C &lt;dbl&gt;, sd_Temperature_C &lt;dbl&gt;,\n#   IQR_Temperature_C &lt;dbl&gt;, mean_Humidity_Percent &lt;dbl&gt;,\n#   median_Humidity_Percent &lt;dbl&gt;, sd_Humidity_Percent &lt;dbl&gt;, …\n\nfind_center_and_spread_grouped(day_df, Seasons)\n\n# A tibble: 4 × 37\n  Seasons mean_Rented_Bike_Count median_Rented_Bike_Count sd_Rented_Bike_Count\n  &lt;fct&gt;                    &lt;dbl&gt;                    &lt;dbl&gt;                &lt;dbl&gt;\n1 Autumn                  22099.                   23350                 6711.\n2 Spring                  17910.                   17590                 8357.\n3 Summer                  24818.                   25572.                7297.\n4 Winter                   5413.                    5498                 1808.\n# ℹ 33 more variables: IQR_Rented_Bike_Count &lt;dbl&gt;, mean_Rainfall_mm &lt;dbl&gt;,\n#   median_Rainfall_mm &lt;dbl&gt;, sd_Rainfall_mm &lt;dbl&gt;, IQR_Rainfall_mm &lt;dbl&gt;,\n#   mean_Snowfall_cm &lt;dbl&gt;, median_Snowfall_cm &lt;dbl&gt;, sd_Snowfall_cm &lt;dbl&gt;,\n#   IQR_Snowfall_cm &lt;dbl&gt;, mean_Temperature_C &lt;dbl&gt;,\n#   median_Temperature_C &lt;dbl&gt;, sd_Temperature_C &lt;dbl&gt;,\n#   IQR_Temperature_C &lt;dbl&gt;, mean_Humidity_Percent &lt;dbl&gt;,\n#   median_Humidity_Percent &lt;dbl&gt;, sd_Humidity_Percent &lt;dbl&gt;, …\n\nfind_center_and_spread_grouped(day_df, Holiday)\n\n# A tibble: 2 × 37\n  Holiday    mean_Rented_Bike_Count median_Rented_Bike_Co…¹ sd_Rented_Bike_Count\n  &lt;fct&gt;                       &lt;dbl&gt;                   &lt;dbl&gt;                &lt;dbl&gt;\n1 Holiday                    12700.                   7184                10504.\n2 No Holiday                 17727.                  19104.                9862.\n# ℹ abbreviated name: ¹​median_Rented_Bike_Count\n# ℹ 33 more variables: IQR_Rented_Bike_Count &lt;dbl&gt;, mean_Rainfall_mm &lt;dbl&gt;,\n#   median_Rainfall_mm &lt;dbl&gt;, sd_Rainfall_mm &lt;dbl&gt;, IQR_Rainfall_mm &lt;dbl&gt;,\n#   mean_Snowfall_cm &lt;dbl&gt;, median_Snowfall_cm &lt;dbl&gt;, sd_Snowfall_cm &lt;dbl&gt;,\n#   IQR_Snowfall_cm &lt;dbl&gt;, mean_Temperature_C &lt;dbl&gt;,\n#   median_Temperature_C &lt;dbl&gt;, sd_Temperature_C &lt;dbl&gt;,\n#   IQR_Temperature_C &lt;dbl&gt;, mean_Humidity_Percent &lt;dbl&gt;, …\n\n\nExploring relationships: plots and correlation values\n\n# categorical vars\nggplot(day_df, aes(x = Rented_Bike_Count)) + geom_density(alpha = 0.5, aes(fill = Seasons)) + ggtitle(\"Rented Bike Count By Season\")\n\n\n\n\n\n\n\nggplot(day_df, aes(x = Rented_Bike_Count)) + geom_density(alpha = 0.5, aes(fill = Holiday)) + ggtitle(\"Rented Bike Count By Holiday\")\n\n\n\n\n\n\n\n\n\n# numerical vars\nggplot(day_df, aes(x = Seasons, y = Rented_Bike_Count, color = Seasons)) + geom_point(position = \"jitter\") + ggtitle(\"Rented Bike Count by Season\")\n\n\n\n\n\n\n\nggplot(day_df, aes(x = Rainfall_mm, y = Rented_Bike_Count)) + geom_point(position = \"jitter\") + ggtitle(\"Rented Bike Count vs. Rainfall\")\n\n\n\n\n\n\n\nggplot(day_df, aes(x = Humidity_Percent, y = Rented_Bike_Count)) + geom_point(position = \"jitter\") + ggtitle(\"Rented Bike Count vs. Humidity\")\n\n\n\n\n\n\n\nggplot(day_df, aes(x = Wind_speed_m_per_s, y = Rented_Bike_Count)) + geom_point(position = \"jitter\") + ggtitle(\"Rented Bike Count vs. Wind Speed\")\n\n\n\n\n\n\n\n# correlation\nday_df |&gt;\n  select(where(is.numeric)) |&gt;\n  correlate()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 9 × 10\n  term  Rented_Bike_Count Rainfall_mm Snowfall_cm Temperature_C Humidity_Percent\n  &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n1 Rent…           NA          -0.239      -0.265        0.753             0.0359\n2 Rain…           -0.239      NA          -0.0231       0.145             0.529 \n3 Snow…           -0.265      -0.0231     NA           -0.267             0.0654\n4 Temp…            0.753       0.145      -0.267       NA                 0.404 \n5 Humi…            0.0359      0.529       0.0654       0.404            NA     \n6 Wind…           -0.193      -0.102       0.0209      -0.261            -0.234 \n7 Visi…            0.166      -0.222      -0.102        0.00234          -0.559 \n8 Dew_…            0.650       0.265      -0.210        0.963             0.632 \n9 Sola…            0.736      -0.323      -0.233        0.550            -0.274 \n# ℹ 4 more variables: Wind_speed_m_per_s &lt;dbl&gt;, Visibility_10m &lt;dbl&gt;,\n#   Dew_point_temperature_C &lt;dbl&gt;, Solar_Radiation_MJ_per_sq_m &lt;dbl&gt;\n\n\nNotes:\n\nThe correlation with Rented Bike Count is strongest for Temperature, Dew Point Temperature, and Solar Radiation.\n\n\n\nSplit the Data\nWe’ll split our data into training and test sets, stratified by the variable Seasons, and then create our 10 folds for 10-fold cross validation.\n\nday_split &lt;- initial_split(day_df, prop=0.75, strata=Seasons)\nday_train &lt;- training(day_split)\nday_test &lt;- testing(day_split)\n\nday_10_fold &lt;- vfold_cv(day_train, 10)\n\n\n\nFitting MLR Models\nCreating 3 recipes:\n\n# recipe #1\nrecipe_1 &lt;- recipe(Rented_Bike_Count ~ ., data=day_train) |&gt;\n  step_date(Date) |&gt;\n  step_mutate(day_type=factor(if_else(Date_dow %in% c('Sat', 'Sun'), 'weekday', 'weekend'))) |&gt;\n  step_rm(Date, Date_dow) |&gt;\n  step_normalize(all_numeric(), -all_outcomes()) |&gt;\n  step_dummy(Seasons, Holiday, day_type)\n\n\n# recipe #2\nrecipe_2 &lt;- recipe(Rented_Bike_Count ~ ., data=day_train) |&gt;\n  step_date(Date) |&gt;\n  step_mutate(day_type=factor(if_else(Date_dow %in% c('Sat', 'Sun'), 'weekday', 'weekend'))) |&gt;\n  step_rm(Date, Date_dow) |&gt;\n  step_normalize(all_numeric(), -all_outcomes()) |&gt;\n  step_dummy(Seasons, Holiday, day_type) |&gt;\n  step_interact(terms = ~starts_with(\"Seasons\")*starts_with(\"Holiday\")) |&gt;\n  step_interact(terms = ~starts_with(\"Seasons\")*Temperature_C) |&gt;\n  step_interact(terms = ~Temperature_C*Rainfall_mm)\n\n\n# recipe #3\nrecipe_3 &lt;- recipe(Rented_Bike_Count ~ ., data=day_train) |&gt;\n  step_date(Date) |&gt;\n  step_mutate(day_type=factor(if_else(Date_dow %in% c('Sat', 'Sun'), 'weekday', 'weekend'))) |&gt;\n  step_rm(Date, Date_dow) |&gt;\n  step_normalize(all_numeric(), -all_outcomes()) |&gt;\n  step_dummy(Seasons, Holiday, day_type) |&gt;\n  step_interact(terms = ~starts_with(\"Seasons\")*starts_with(\"Holiday\")) |&gt;\n  step_interact(terms = ~starts_with(\"Seasons\")*Temperature_C) |&gt;\n  step_interact(terms = ~Temperature_C*Rainfall_mm) |&gt;\n  step_poly(Rainfall_mm, Snowfall_cm, Temperature_C, Humidity_Percent, Wind_speed_m_per_s, Visibility_10m, Dew_point_temperature_C, Solar_Radiation_MJ_per_sq_m)\n\nSet up a linear model:\n\nday_mod &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nTrain with 10-fold CV with each recipe and look at the metrics for all:\n\nday_CV_fits_1 &lt;- workflow() |&gt;\n  add_recipe(recipe_1) |&gt;\n  add_model(day_mod) |&gt;\n  fit_resamples(day_10_fold)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\n\n\n\n\nday_CV_fits_2 &lt;- workflow() |&gt;\n  add_recipe(recipe_2) |&gt;\n  add_model(day_mod) |&gt;\n  fit_resamples(day_10_fold)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\nday_CV_fits_3 &lt;- workflow() |&gt;\n  add_recipe(recipe_3) |&gt;\n  add_model(day_mod) |&gt;\n  fit_resamples(day_10_fold)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x10\n\n\n\n\nrbind(day_CV_fits_1 |&gt; collect_metrics(),\n day_CV_fits_2 |&gt; collect_metrics(),\n day_CV_fits_3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   3347.       10 204.     Preprocessor1_Model1\n2 rsq     standard      0.882    10   0.0152 Preprocessor1_Model1\n3 rmse    standard   3083.       10 325.     Preprocessor1_Model1\n4 rsq     standard      0.898    10   0.0227 Preprocessor1_Model1\n5 rmse    standard   2911.       10 278.     Preprocessor1_Model1\n6 rsq     standard      0.912    10   0.0184 Preprocessor1_Model1\n\n\nBest model: recipe #1!\nNow let’s train using recipe 1 with the entire training dataset:\n\nday_wfl &lt;- workflow() |&gt;\n  add_recipe(recipe_1) |&gt;\n  add_model(day_mod)\n\nfit &lt;- day_wfl |&gt;\n  last_fit(day_split)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n# test set metrics\nfit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2587.    Preprocessor1_Model1\n2 rsq     standard       0.934 Preprocessor1_Model1\n\n\nFinally, let’s train on the entire dataset:\n\n# fit on all data\nfull_fit &lt;-day_wfl |&gt;\n  fit(day_df)\n\nfull_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 26 × 5\n   term                        estimate std.error statistic  p.value\n   &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                    7918.     1347.     5.88  1.00e- 8\n 2 Rainfall_mm                   -2251.      221.   -10.2   1.91e-21\n 3 Snowfall_cm                    -335.      179.    -1.87  6.22e- 2\n 4 Temperature_C                 -2311.     3221.    -0.718 4.74e- 1\n 5 Humidity_Percent              -2127.     1191.    -1.79  7.50e- 2\n 6 Wind_speed_m_per_s             -545.      189.    -2.88  4.27e- 3\n 7 Visibility_10m                  288.      283.     1.02  3.09e- 1\n 8 Dew_point_temperature_C        7214.     3772.     1.91  5.67e- 2\n 9 Solar_Radiation_MJ_per_sq_m    3223.      319.    10.1   3.94e-21\n10 Date_monthFeb                 -1947.      846.    -2.30  2.19e- 2\n# ℹ 16 more rows\n\n\n\n\nHW9: Modeling Practice\nLet’s train a few other types of models and see how the results compare! We’ll first find the best tuning parameters for each type of model using 10-fold cross validation. Then we’ll train our best model for each model type on the entire training dataset.\nFirst up: LASSO model\n\n# create LASSO recipe using recipe from HW8\nLASSO_recipe &lt;- recipe(Rented_Bike_Count ~ ., data=day_train) |&gt;\n  step_date(Date) |&gt;\n  step_mutate(day_type=factor(if_else(Date_dow %in% c('Sat', 'Sun'), 'weekday', 'weekend'))) |&gt;\n  step_rm(Date, Date_dow, Date_month, Date_year) |&gt;\n  step_normalize(all_numeric(), -all_outcomes()) |&gt;\n  step_dummy(Seasons, Holiday, day_type)\n\n# use LASSO model from parsnip\nLASSO_model &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\n# create LASSO workflow\nLASSO_wfl &lt;- workflow() |&gt;\n  add_recipe(LASSO_recipe) |&gt;\n  add_model(LASSO_model)\n\n# use a grid search for tuning parameter\nLASSO_grid &lt;- LASSO_wfl |&gt;\n  tune_grid(resamples = day_10_fold) \n\nWarning: package 'glmnet' was built under R version 4.4.2\n\n# use RMSE to determine the best model (i.e. that with the best tuning parameter value)\nLASSO_best &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\n\nLASSO_final_wfl &lt;- LASSO_wfl |&gt;\n  finalize_workflow(LASSO_best)\n\nLASSO_final_wfl |&gt;\n  last_fit(day_split) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3815.    Preprocessor1_Model1\n2 rsq     standard       0.864 Preprocessor1_Model1\n\nLASSO_final &lt;- LASSO_final_wfl |&gt;\n  fit(day_train)\n\nNotes:\n\nAll RMSE values were the same during parameter tuning, so the parameter value appears to have no effect on the model performance.\n\nRegression Tree model\n\n# same recipe works for tree\ntree_recipe &lt;- LASSO_recipe\n\n# use decision tree model from parsnip\ntree_model &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# create tree workflow\ntree_wfl &lt;- workflow() |&gt;\n  add_recipe(tree_recipe) |&gt;\n  add_model(tree_model)\n\n# use a grid search for tuning parameter\ntree_grid &lt;- tree_wfl |&gt; \n  tune_grid(resamples = day_10_fold)\n\n# use RMSE to determine the best model (i.e. that with the best tuning parameter value)\ntree_best &lt;- select_best(tree_grid, metric=\"rmse\")\n\ntree_final_wfl &lt;- tree_wfl |&gt;\n  finalize_workflow(tree_best)\n\ntree_final_wfl |&gt;\n  last_fit(day_split) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3352.    Preprocessor1_Model1\n2 rsq     standard       0.896 Preprocessor1_Model1\n\ntree_final &lt;- tree_final_wfl |&gt;\n  fit(day_train)\n\nNotes:\n\nmuch better performance from the decision tree model than LASSO!\n\nBagged Tree model\n\n# same recipe works for bagged tree\nbag_recipe &lt;- LASSO_recipe\n\n# use bagged tree model from parsnip\nbag_model &lt;- bag_tree(tree_depth = 7, min_n = 10, cost_complexity = tune()) |&gt;\n set_engine(\"rpart\") |&gt;\n set_mode(\"regression\")\n\n# create bagged tree workflow\nbag_wfl &lt;- workflow() |&gt;\n add_recipe(bag_recipe) |&gt;\n add_model(bag_model)\n\n# use a grid search for tuning parameter\nbag_grid &lt;- bag_wfl |&gt;\n tune_grid(resamples = day_10_fold,\n grid = grid_regular(cost_complexity(),\n levels = 10))\n\n# use RMSE to determine the best model (i.e. that with the best tuning parameter value)\nbag_best &lt;- select_best(bag_grid, metric=\"rmse\")\n\nbag_final_wfl &lt;- bag_wfl |&gt;\n  finalize_workflow(bag_best)\n\nbag_final_wfl |&gt;\n  last_fit(day_split) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3196.    Preprocessor1_Model1\n2 rsq     standard       0.902 Preprocessor1_Model1\n\nbag_final &lt;- bag_final_wfl |&gt;\n  fit(day_train)\n\nNotes:\n\nThe bagged tree model did even better than the single tree.\n\nRandom Forest model\n\n# same recipe works for random forest\nrf_recipe &lt;- LASSO_recipe\n\n# use random forest model from parsnip\nrf_model &lt;- rand_forest(mtry = tune()) |&gt;\n set_engine(\"ranger\") |&gt;\n set_mode(\"regression\")\n\n# create random forest workflow\nrf_wfl &lt;- workflow() |&gt;\n add_recipe(rf_recipe) |&gt;\n add_model(rf_model)\n\n# use a grid search for tuning parameter\nrf_grid &lt;- rf_wfl |&gt;\n tune_grid(resamples = day_10_fold,\n grid = 10)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWarning: package 'ranger' was built under R version 4.4.2\n\n# use RMSE to determine the best model (i.e. that with the best tuning parameter value)\nrf_best &lt;- select_best(rf_grid, metric=\"rmse\")\n\nrf_final_wfl &lt;- rf_wfl |&gt;\n  finalize_workflow(rf_best)\n\nrf_final_wfl |&gt;\n  last_fit(day_split) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2734.    Preprocessor1_Model1\n2 rsq     standard       0.930 Preprocessor1_Model1\n\nrf_final &lt;- rf_final_wfl |&gt;\n  fit(day_train)\n\nNotes:\n\nThe random forest is the best performing model, according to the test set.\n\nNow that we’ve tuned a model from each family of models, and trained each on the entire training set, let’s compare using the test dataset and RMSE, MAE metrics:\n\n# get final MLR model\nMLR_final &lt;- day_wfl |&gt;\n  fit(day_train)\n\n# get all metrics\nmetrics &lt;- rbind(\n  MLR_RMSE = MLR_final |&gt; predict(day_test) |&gt; pull() |&gt; rmse_vec(truth = day_test$Rented_Bike_Count),\n  MLR_MAE = MLR_final |&gt; predict(day_test) |&gt; pull() |&gt; mae_vec(truth = day_test$Rented_Bike_Count),\n  \n  LASSO_RMSE = LASSO_final |&gt; predict(day_test) |&gt; pull() |&gt; rmse_vec(truth = day_test$Rented_Bike_Count),\n  LASSO_MAE = LASSO_final |&gt; predict(day_test) |&gt; pull() |&gt; mae_vec(truth = day_test$Rented_Bike_Count),\n  \n  tree_RMSE = tree_final |&gt; predict(day_test) |&gt; pull() |&gt; rmse_vec(truth = day_test$Rented_Bike_Count),\n  tree_MAE = tree_final |&gt; predict(day_test) |&gt; pull() |&gt; mae_vec(truth = day_test$Rented_Bike_Count),\n  \n  bagged_tree_RMSE = bag_final |&gt; predict(day_test) |&gt; pull() |&gt; rmse_vec(truth = day_test$Rented_Bike_Count),\n  bagged_tree_MAE = bag_final |&gt; predict(day_test) |&gt; pull() |&gt; mae_vec(truth = day_test$Rented_Bike_Count),\n  \n  random_forest_RMSE = rf_final |&gt; predict(day_test) |&gt; pull() |&gt; rmse_vec(truth = day_test$Rented_Bike_Count),\n  random_forest_MAE = rf_final |&gt; predict(day_test) |&gt; pull() |&gt; mae_vec(truth = day_test$Rented_Bike_Count))\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\", : prediction from rank-deficient fit; consider predict(.,\nrankdeficient=\"NA\")\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\", : prediction from rank-deficient fit; consider predict(.,\nrankdeficient=\"NA\")\n\ncolnames(metrics) = c(\"Error\")\n\nmetrics\n\n                      Error\nMLR_RMSE           2586.575\nMLR_MAE            1937.023\nLASSO_RMSE         3815.091\nLASSO_MAE          3116.484\ntree_RMSE          3352.170\ntree_MAE           2532.949\nbagged_tree_RMSE   3026.223\nbagged_tree_MAE    2408.673\nrandom_forest_RMSE 2746.489\nrandom_forest_MAE  2259.566\n\n\nLet’s also see a summary of each final model:\n\nMLR_final |&gt; extract_fit_parsnip() |&gt; tidy()\n\n# A tibble: 26 × 5\n   term                        estimate std.error statistic  p.value\n   &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                    7295.     1628.     4.48  1.15e- 5\n 2 Rainfall_mm                   -2509.      277.    -9.05  5.05e-17\n 3 Snowfall_cm                    -418.      220.    -1.90  5.88e- 2\n 4 Temperature_C                 -2141.     3787.    -0.565 5.72e- 1\n 5 Humidity_Percent              -2017.     1420.    -1.42  1.57e- 1\n 6 Wind_speed_m_per_s             -681.      230.    -2.96  3.37e- 3\n 7 Visibility_10m                  234.      349.     0.669 5.04e- 1\n 8 Dew_point_temperature_C        6588.     4395.     1.50  1.35e- 1\n 9 Solar_Radiation_MJ_per_sq_m    3297.      384.     8.59  1.14e-15\n10 Date_monthFeb                 -1693.     1106.    -1.53  1.27e- 1\n# ℹ 16 more rows\n\nLASSO_final |&gt; extract_fit_parsnip() |&gt; tidy()\n\n# A tibble: 14 × 3\n   term                        estimate  penalty\n   &lt;chr&gt;                          &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                  16858.  6.71e-10\n 2 Rainfall_mm                  -2138.  6.71e-10\n 3 Snowfall_cm                   -397.  6.71e-10\n 4 Temperature_C                  662.  6.71e-10\n 5 Humidity_Percent             -1156.  6.71e-10\n 6 Wind_speed_m_per_s            -804.  6.71e-10\n 7 Visibility_10m                 -96.7 6.71e-10\n 8 Dew_point_temperature_C       3641.  6.71e-10\n 9 Solar_Radiation_MJ_per_sq_m   4029.  6.71e-10\n10 Seasons_Spring               -4944.  6.71e-10\n11 Seasons_Summer               -3962.  6.71e-10\n12 Seasons_Winter               -7628.  6.71e-10\n13 Holiday_No.Holiday            3069.  6.71e-10\n14 day_type_weekend              2426.  6.71e-10\n\ntree_final |&gt; extract_fit_engine() |&gt; rpart.plot::rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\nbag_model_extracted &lt;- extract_fit_engine(bag_final)\nbag_model_extracted$imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip()\n\n\n\n\n\n\n\n\nNotes:\n\nThe best family of models that we tested is the random forest! Now we can train that model on the entire dataset to get a final model:\n\n\nrf_final_model &lt;- rf_final_wfl |&gt;\n  fit(day_df)\n\nrf_final_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~10L,      x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5,      1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             10 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       7540544 \nR squared (OOB):                  0.9236379"
  }
]